\documentclass{article}

\usepackage{likeicml} 

% For figures
\usepackage{graphicx}
\usepackage{subfigure} 

% For citations
\usepackage{mlapa}

\icmltitlerunning{AUSDM Challenge 2009: Team ``Barneso''}

\begin{document} 

\twocolumn[
\icmltitle{Team ``Barneso'' Report for the AUSDM Ensembling Challenge 2009}

\icmlauthor{Jeremy Barnes}{jeremy@barneso.com}

\vskip 0.3in

(Shameless plug: I am looking for consulting work: Recommendation Engines, Data Mining, Machine Learning or Computational Linguistics).

\vskip 0.3in
]

\begin{abstract} 

We describe how we combined 1151 ``black box'' models into a more powerful ensemble predictor.  Several models that achieved significant success on the AUC task were produced: a deep neural network model, a gated classifier and a multiple logistic regression model.  Further improvement was achieved by adding features derived from the model input, and by adding unsupervised features from decomposing the model inputs (using a SVD or denoising auto-encoders).

On the large AUC task, the baseline performance\footnote{The average of the most accurate 20 models over a held-out set of 20\% of the training set.  These scores are not the official scores, which are calculated on data for which the labels were not available.} of 0.1635 was reduced to 0.1461.  On the more difficult medium task, the baseline performance of 0.3384 was reduced to 0.3144.  On the simpler small task, the baseline of 0.0597 was reduced to 0.0571.

Less attention was given to the RMSE task.  On the small task, the baseline of 0.4398\footnote{These RMSE scores are calculated on a ratings scale of [-1,1] not [1000,5000] as in the competition, and so are not directly comparable.} to the was reduced to 0.4381.  On the medium task, the baseline of 0.4307 was reduced to 0.4277.  On the large task, the baseline of 0.4419 was improved to 0.4385.  It was observed that very little improvement was possible on this task.

The ``black box'' nature of the competition and the underlying noise in the labels (to which the RMSE score is particularly sensitive) make progress difficult.  An alternative framework for ensembling is discussed which, whilst placing more requirements on model builders, would likely lead to better improvement in the ensembles.

\end{abstract}

\section{Introduction}

The AUSDM ensembling challenge is based upon the NetFlix Prize \cite{NetFlixPrize} which recently finished.

\subsection{NetFlix Prize}

 The goal of this competition was to predict the rating (from 1 to 5) that a person would give to a film on a particular date, given a training dataset that contained examples of predictions that had already been made.  The score was evaluated with the Root Mean Squared Error (RMSE):

$$
E = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{x}_i)^2} \label{RMSE}.
$$

A ``probe'' dataset was also made available, which included 1.4 million (user, date, rating) triplets that were not included in the training set.  Finally, a third dataset was provided; this dataset included (user, date) pairs, and the goal of the competition was to predict the rating for each of these pairs.

The leading entries in this competition were from coalitions of collaborating teams.  The teams would independently produce models that were trained only on the training set.  These models would then be run to predict the values in both the probe and testing datasets.  These predicted results would then be exchanged within the group of collaborators, and a final blended model would be produced, with the parameters for the blend learned from the probe set.

\subsection{AUSDM Competition}

The organisers of the AUSDM competition first approached the two leading coelitions and obtained from them the results of all of their models (some 1151 in total) on the probe set.  From this large combined set of data, random sampling was used to produce twelve datasets, with three data sizes (Small, Medium and Large as decribed in table \ref{problems}), two problem types (AUC and RMSE) and two datasets for each (a training set including target values, and a testing set with the target values removed).

\begin{table}[t]
\caption{Problem sizes and row counts.}
\label{problems}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrrr}
\hline
\abovespace\belowspace
Size & Training & Testing & Models & Size \\
\hline
\abovespace
Small    & 15,000 & 15,000 & 200 & 300MB \\
Medium   & 25,000 & 25,000 & 250 &  25MB \\
\belowspace
Large    & 50,000 & 50,000 & 1151 & 15MB \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Note that all information about the movies, users, dates and models was removed from the dataset.  This was presumably due to the probe set being publicly available for the duration of the NetFlix Prize; it would be very easy to use the probe scores to improve performance (either accidentally or on purpose).  The result was to make the contest a complete ``black box'', and preclude the use of any side-channel information.  This point will be discussed below.

\subsubsection{RMSE Task}

The RMSE problem in the AUSDM Challenge was identical to that in the NetFlix Prize: minimize the RMSE in equation \ref{RMSE}.

\subsubsection{AUC Task}

The AUC task was a binary ranking problem.  Two ratings were selected (for example, 1 star and 5 stars), and only rows with one of these ratings were samples.  These two rows were then assigned the labels $+1$ and $-1$.  The goal was to minimize the AUC score, which is the average false rate (the sum of the average false positive rate over negative examples and the average false negative rate over positive examples).

Intuitively, the goal of this task is to separate the two classes as much as possible.  For this task, only the order of the scores mattered, not their magnitudes.  As a result, it is possible to use the RMSE score as a surrogate to calculate the AUC.

Table \ref{auc} shows details of the three tasks and the likely correspondence between the $\pm 1$ values and the number of stars.  A baseline AUC score using the average RMSE of the 20 models with the highest AUC score is also provided\footnote{I used the AOC (area over the curve, $\mathrm{AOC} = 1 - \mathrm{AUC}$) as my score so that the score could be interpreted as an error.}
The medium task is much more difficult than the large task, which is more difficult than the small task.  On the other hand, the more difficult tasks had more scope for improvement than the small task.

\begin{table}[t]
\caption{AUC Task and guessed correspondence between $\pm 1$ and star values.}
\label{auc}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrr}
\hline
\abovespace\belowspace
Size & -1 & +1 & AOC Top 20 \\
\hline
\abovespace
Small    & 1 & 5 & 0.0597 \\
Medium   & 2 & 3 & 0.3384 \\
\belowspace
Large    & 2 & 4 & 0.1635 \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

As a one-person team with limited time and labour, I choose to concentrate mostly on this task.  I consider that this represents better the real-world application of recommendation engines (for example, NetFlix presumably wants to optimize the probability that someone who sees a recommendation rents the film\footnote{Or rents the film and doesn't hate it}, as this increases revenue by making people take out more profitable subscriptions and reducing the chance that they will let their subscription lapse.

\subsection{Problem Analysis}

Clearly, the NetFlix competition 


\subsection{Competition}

About the competition

\section{Decompositions}

One of the interesting aspects of this competition is the complete lack of any
side-channel source of information\footnote{For example, in the original NetFlix
competition the identities of the films were known, which allowed the
possibility to obtain further information about the film from the internet.}.

In order to determine the accuracy of a prediction of a particular model, it is
therefore necessary to use information about predictions from the other models.

\subsection{Singular Value Decomposition}

The most basic

Analyzing the singular vectors generated, the 


An analysis of the utility of these features showed that only the first couple
(which corresponded to the mean and bias of the data) had any significant
correlation with the 

\subsection{Denoising Auto-Encoder Decomposition}

One of my goals for this competition was to explore the use of deep neural network architectures.  The architecture that I choose was that of denoising autoencoders.

The goal of a standard auto encoder is to learn the identity function: two functions $f(\cdot)$ and $g(\cdot)$ such that $f(g(\mathbf{x})) = \mathbf{x}$.  Generally, in order to be interesting an auto-encoder will include some kind of restriction: for example, the number of dimensions in the range of $f$ is smaller than the number of dimentions in its domain.

In neural networks, the most commonly used formulation shares an activation matrix $\mathbf{W}$ between the forward and reverse directions:

$$
f(\mathbf{x}) = t(\mathbf{Wx} + \mathbf{b})
$$
$$
g(\mathbf{x}) = t(\mathbf{W}^T\mathbf{x} + \mathbf{c})
$$

The output of $f(x)$ is the \emph{encoded} form of the input $\mathbf{x}$, and the function $f$ is the \emph{encoder function}.  Similarly, $g$ is the \emph{decoder} function.  When we train the auto encoder over a 

In some circumstances, it may encode nearly the same information as in $\mathbf{x}$.  For example, if $t$ is the identity function and $\mathbf{W}$ is the left-singular matrix of a set of 

When $t$ is the identity function, this formulation reduces (essentially, see below) to the singular value decomposition.

When $t(\cdot)$ is a non-linear squashing function such as (in my case) $t(\mathbf{x}) = \tanh(\mathbf{x})$, we can use this neural network to learn a nonlinear decomposition.

Note that these auto-encoders can be stacked one on top of the other, so that we apply all of the forward functions one after the other and then all of the reverse directions in the opposite order.

In order to make the auto encoder generalize (and learn higher level features), we need to do one of two things:

\begin{itemize}
\item Add noise to the input, but train the network to reconstruct the non-noisy input (remove the noise).  This is where it becomes a ``denoising'' auto-encoder;
\item Create an information bottleneck, whereby the number of features from which the input is reconstructed is lower than the number of features in the input.
\end{itemize}

\subsubsection{Addition of noise}

\subsubsection{Information bottleneck}

In addition to adding noise as described above, I also created an information bottleneck.

\section{Derived Features}

In order to make it easier for the classifiers to work, several features were derived from the outputs of the models:

\begin{itemize}
\item The minimum, maximum, mean and standard deviation of the model outputs;
\item For of the 10 highest ranking models:
  \begin{itemize}
    \item The minimum, maximum and mean value;
    \item The difference between the spread of values over these 10 models and the spread of values over all of the models;
    \item For each of the 10 models, the output of the model;
    \item For each of the 10 models, the number of standard deviations from the mean;
    \item The difference between the output of this model and the closest integer (for example, 4.513 which corresponds to 4513 in the data files would return 0.487);
    \item If a decomposition was done, the error of the reconstruction of this model by the decomposition;
  \end{itemize}
\item The output of the decomposition (SVD or Denoising Auto-Encoder), if there was any;
\item The overall RMS error of the decomposition on this example.

\end{itemize}


\section{Models}

There were three main models used in the entry:

\begin{itemize}
\item A multiple regression model;
\item A gated classifier model;
\item A deep neural network model.
\end{itemize}

In addition, several other models were tried:

\begin{itemize}
\item A boosting model
\item A standard classifier model
\end{itemize}

Each will be described briefly in the following sections.

\subsection{Multiple Regression Models}

\subsection{Gated Classifier Models}

\subsection{Deep Neural Network Model}

This model started with the denoising autoencoders

\section{Results Generation}

In order to generate results, a large number of predictors were trained individually based upon the models above.  Each of these predictors was trained on 80\% of the training data, with the other 20\% (always the same part) held out in order to train the final blending model.  Once each model was run, it created:

\begin{enumerate}
\item A blending results file containing the model's unbiased prediction for each entry of the 20\% of final blending data held out;
\item A submission results file containing the model's prediction for each entry of the scoring set (for which labels weren't available).
\end{enumerate}

Once all of the files were available, they were combined a final blender, and this result became the submitted output.

\subsection{Models Used}

There were four main classes of models that were used to contribute to the final score.

\subsubsection{Multiple Regression}

The multiple regression models turned out to be the most powerful models, particularly in the RMSE task.  This is largely due to their ability to reject noise due to their inherent smoothness, the regularization provided by ridge regression and the smoothing provided by the random selection of features and examples.

\subsubsection{Deep Neural Networks}

The very first results obtained from the deep neural network were extremely good, and placed quite high on the leaderboard.  However, all attempts to reproduce these results failed\footnote{The code that generated these results had a bug in it which caused the execution to be non-deterministic}: the training error of the deep net stopped improving far before the point observed on the first run.  The results from the deep net are included as they tend to be more diverse than the other models, but they did not contribute greatly to the final score.

\subsubsection{Gated Merger}

Several models of the gated merger were tried.  This model tended to perform reasonably well for the AUC task, but poorly for the RMSE task.  Presumably, this is because the two-stage nature of the model caused the noise to be amplified between the stages.

The models differed in which decomposition they used (no decomposition, the SVD or the denoising autoencoders), whether or not they used extra features, and the technique used for the final score once the confidence-modified values had been produced.

\subsubsection{Classifier Models}

For the RMSE data, two classifier models were used.  One, \texttt{rtrees} used a random forest of 200 regression trees.  The other, \texttt{mclass}, learnt a binary classifier for each discrete movie rating (1, 2, 3, 4 or 5 stars) using a random forest of 500 decision trees, and combined these predictions using linear regression.  Neither model performed particularly well.


\subsection{Final Blending}

Final blending was performed using a cross-validation training on the 20\% held out data.  The held out data was broken into 10 folds, and 10 different multiple regression models were trained, each leaving out one fold.  The performance of the merged model on the entire 20\% held out was then evaluated.  The submission results were generated by running each of the 10 multiple regression models and averaging the results.

This strategy was adapted in order to reduce the impact that a ``rogue'' model (with a high error rate) would have on the final blend.  It is unlikely that performing (yet another) round of blending of already blended models would significantly improve the results.  Straight linear blending worked just as well, especially for the RMSE task.

The final blending provided a substantial improvement in the AUC task.  On the RMSE task, final blending was ineffectual: the score of the blended result was slightly worse than the best individual result\footnote{The submitted results were still the blended ones, however, as they should be more resistant to the selection bias in the validation set}.

This is probably due to there not being enough diversity in the models blended: there were only a few really distinct models, and these frequently used the same features (from the DNAE, the SVD and the derived features).

\subsection{Implementation}

In practice, this competition turned out (for me, anyway) to be as much about software engineering as about data mining.

The biggest reason for this was the amount of noise in the data compared to the size of the data itself.  The variation over even very sparesly parameterized models such as linear regression was extremely large, and for non-linear models (especially those using algorithms such as boost) was even worse.  As a result, it was necessary to run each model as many times as possible over different random subsets of the data and average the results (for example, 5000 decision trees were trained for the random forest classifier in the gated model).

Due to the limited hardware resources available\footnote{One quad core ``hyperthreaded'' (8 virtual cores) desktop machine with 6GB of RAM, one dual core laptop with 2GB of RAM} and the large number of tasks, it was necessary that the software be both memory and CPU efficient.
The entire code was vectorized to take advantage of the vector unit, multi-threaded\footnote{On the desktop machine, 8 threads were run to fully exploit the hyper-threaded processor} and the bottlenecks were profiled and carefully optimized.  Single precision arithmetic was used wherever possible\footnote{It is frequently not possible.  For example, whenever accumulating a series of numbers it is necessary to accumulate in double precision even if the numbers being accumulated are only in single precision.} due to its advantage in execution speed.

In addition, parameters were saved using as small a precision as possible and care was taken not to duplicate memory when splitting datasets into training and validation sets.

The fact that there were six different tasks (AUC and RMSE for the small, medium and large datasets) also increased the amount of CPU time and engineering work required, as anything that was model or dataset-specific needed to be parameterized out and a method found to calculate the parameter for the task at hand.

In the end, about 5,000 lines of C++ code were written to handle the competition directly, and about 15,000 lines added to the underlying machine learning library (primarily the code to perform Ridge Regression and the denoising auto-encoder routines).  The entire set of results could be reproduced in about 24 hours on the desktop machine.

\subsection{Platform}

The software was developed on Linux.  The only significant external libraries used were LAPACK and BLAS for the linear algebra routines.

\subsection{Open Source}

The source code for this submission is available.  The machine learning library used to perform the heavy lifting is available at http://bitbucket.org/jeremy\_barnes/jml/.  The source code of the actual ausdm submission is abailable at http://github.com/jeremybarnes/ausdm.  Both are available under the Affero GNU Public License version 3.

Some of the data files used in the building of the results are available (the full set of files could not be provided as they are too large for the version control hosting service that is used).  These could be used to verify the results and to perform further experiments.  Please note that these files are only available to the AUSDM or NetFlix challenge participants, as they contain ranking values from the original NetFlix dataset that are not freely redistributable.

\section{Discussion}

\subsection{Difficulty Values of Rows}

It is instructive to break the dataset into different types of rows, based upon the ease with which a prediction can be made.  For RMSE, we could define a row to be \emph{impossible} if the predictions of all of the models are closer to the target value than the

\begin{table}[t]
\caption{RMSE large set broken down by row difficulty.  In the last column, we see that 1/3 of the MSE comes from the impossible positions, which account for just 5.7\% of the data.}
\label{rowtypesrmse}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrr}
\hline
\abovespace\belowspace
Category & Freq & Avg MSE & Total MSE \\
\hline
\abovespace
Automatic     & 0.329 & 0.019 & 0.0063 \\
Possible      & 0.614 & 0.198 & 0.1216 \\
\belowspace
Impossible    & 0.057 & 1.189 & 0.0678 \\
\hline
Total         & 1.000 & 0.195 & 0.1953 \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}




\subsection{Black Box Model}


\subsection{}


\subsection{Directions for Further Investigation}

\begin{itemize}
\item Non-linear but noise resistant regression models
\end{itemize}


\section{Conclusion}


% Acknowledgements should only appear in the accepted version. 
\section*{Acknowledgments} 

Thanks to Phil Brierley for organizing and running the competition, and to my family for putting up with the late evenings.

The \LaTeX style file for this report is based upon that for ICML 2009, by P Langley, Terran Lane, Jennifer Dy, Kristian Kersting and Ricardo Silva.


\bibliography{report}
\bibliographystyle{mlapa}

\end{document} 

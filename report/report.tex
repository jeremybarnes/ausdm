\documentclass{article}

\usepackage{likeicml} 

% For figures
\usepackage{graphicx}
\usepackage{subfigure} 

% For citations
\usepackage{mlapa}

\icmltitlerunning{AUSDM Challenge 2009: Team ``Barneso''}

\begin{document} 

\twocolumn[
\icmltitle{Team ``Barneso'' Report for the AUSDM Ensembling Challenge 2009}

\icmlauthor{Jeremy Barnes}{jeremy@barneso.com}

\vskip 0.3in

(Shameless plug: I am looking for consulting work: Recommendation Engines, Data Mining, Machine Learning or Computational Linguistics).

\vskip 0.3in
]

\begin{abstract} 

We describe how we combined 1151 ``black box'' models into a more powerful ensemble predictor.  Several models that achieved significant success on the AUC task were produced: a deep neural network model, a gated classifier and a multiple logistic regression model.  Further improvement was achieved by adding features derived from the model input, and by adding unsupervised features from decomposing the model inputs (using a SVD or denoising auto-encoders).

On the large AUC task, the baseline AOC\footnote{AOC = 1 - AUC} performance\footnote{The average of the most accurate 20 models over a held-out set of 20\% of the training set.  These scores are not the official scores, which are calculated on data for which the labels were not available.} of 0.1635 was improved to 0.1461.  On the more difficult medium task, the baseline performance of 0.3384 was improved to 0.3144.  On the simpler small task, the baseline of 0.0597 was improved to 0.0571.

Less attention was given to the RMSE task.  On the small task, the baseline of 0.4398\footnote{These RMSE scores are calculated on a ratings scale of [-1,1] not [1000,5000] as in the competition, and so are not directly comparable.} to the was reduced to 0.4381.  On the medium task, the baseline of 0.4307 was reduced to 0.4277.  On the large task, the baseline of 0.4419 was improved to 0.4385.  It was observed that very little improvement was possible on this task.

The ``black box'' nature of the competition and the underlying noise in the labels (to which the RMSE score is particularly sensitive) make progress difficult.  An alternative framework for ensembling is discussed which, whilst placing more requirements on model builders, would likely lead to better improvement in the ensembles.

\end{abstract}

\section{Introduction}

The AUSDM ensembling challenge was a competition that ran for approximately six weeks in October and November, 2009.  The goal of the challenge was to combine existing personalised movie rating models in order to achieve a better result than the original model.  The dataset was derived from the NetFlix Prize \cite{NetFlixPrize}.

\subsection{NetFlix Prize}

The goal of this competition was to predict the rating (from 1 to 5) that a person would give to a film on a particular date, given a training dataset that contained examples of predictions that had already been made.  The score was evaluated with the Root Mean Squared Error (RMSE):

\begin{equation}
E = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{x}_i)^2} \label{RMSE}.
\end{equation}

A ``probe'' dataset was also made available, which included 1.4 million (user, date, rating) triplets that were not included in the training set.  Finally, a third dataset was provided; this dataset included (user, date) pairs, and the goal of the competition was to predict the rating for each of these pairs.

The leading entries in this competition were from coalitions of collaborating teams.  The teams would independently produce models that were trained only on the training set.  These models would then be run to predict the values in both the probe and testing datasets.  These predicted results would then be exchanged within the group of collaborators, and a final blended model would be produced, with the parameters for the blend learned from the probe set.

\subsection{AUSDM Competition}

The organisers of the AUSDM competition first approached the two leading coalitions and obtained from them the results of all of their models (some 1151 in total) on the probe set.  From this large combined set of data, random sampling was used to produce twelve datasets, with three data sizes (Small, Medium and Large as decribed in table \ref{problems}), two problem types (AUC and RMSE) and two datasets for each (a training set including target values, and a testing set with the target values removed).

\begin{table}[t]
\caption{Problem sizes and row counts.}
\label{problems}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrrr}
\hline
\abovespace\belowspace
Size & Training & Testing & Models & Size \\
\hline
\abovespace
Small    & 15,000 & 15,000 & 200 & 300MB \\
Medium   & 25,000 & 25,000 & 250 &  25MB \\
\belowspace
Large    & 50,000 & 50,000 & 1151 & 15MB \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Note that all information about the movies, users, dates and models was removed from the dataset.  This was presumably due to the probe set being publicly available for the duration of the NetFlix Prize; it would be very easy to use the probe scores to improve performance (either accidentally or on purpose).  The result was to make the contest a complete ``black box'', and preclude the use of any side-channel information.  This point will be discussed below.

It is unknown if any of the probe sets collected were in fact blended versions of other models.  If they were, then the baseline results would be artificially inflated\footnote{The results of the SVD decomposition in section \ref{svd-decomposition} show that the models are mostly linearly independent, so it is likely that little blending had occurred}.

\subsubsection{RMSE Task}

The RMSE problem in the AUSDM Challenge was identical to that in the NetFlix Prize: minimize the RMSE in equation \ref{RMSE}.

\subsubsection{AUC Task}

The AUC task was a binary ranking problem.  Two ratings were selected (for example, 1 star and 5 stars), and only rows with one of these ratings were samples.  These two rows were then assigned the labels $+1$ and $-1$.  The goal was to minimize the AUC score, which is the average false rate (the sum of the average false positive rate over negative examples and the average false negative rate over positive examples).

Intuitively, the goal of this task is to separate the two classes as much as possible.  For this task, only the order of the scores mattered, not their magnitudes.  As a result, it is possible to use the RMSE score as a surrogate to calculate the AUC.

Table \ref{auc} shows details of the three tasks and the likely correspondence between the $\pm 1$ values and the number of stars.  A baseline AUC score using the average RMSE of the 20 models with the highest AUC score is also provided\footnote{I used the AOC (area over the curve, $\mathrm{AOC} = 1 - \mathrm{AUC}$) as my score so that the score could be interpreted as an error.}
The medium task is much more difficult than the large task, which is more difficult than the small task.  On the other hand, the more difficult tasks had more scope for improvement than the small task.

\begin{table}[t]
\caption{AUC Task and guessed correspondence between $\pm 1$ and star values.}
\label{auc}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrr}
\hline
\abovespace\belowspace
Size & -1 & +1 & AOC Top 20 \\
\hline
\abovespace
Small    & 1 & 5 & 0.0597 \\
Medium   & 2 & 3 & 0.3384 \\
\belowspace
Large    & 2 & 4 & 0.1635 \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

As a one-person team with limited time and labour, I choose to concentrate mostly on this task.  I consider that this represents better the real-world application of recommendation engines (for example, NetFlix presumably wants to optimize the probability that someone who sees a recommendation rents the film\footnote{Or rents the film and doesn't hate it}, as this increases revenue by making people take out more profitable subscriptions and reducing the chance that they will let their subscription lapse.

\subsection{Problem Analysis}

Clearly, the NetFlix competition 


\section{Decompositions}

One of the interesting aspects of this competition is the complete lack of any
side-channel source of information\footnote{For example, in the original NetFlix
competition the identities of the films were known, which allowed the
possibility to obtain further information about the film from the internet.}.  Indeed, even movies that could normally be used to determine the accuracy of the underlying models (such as the amount of information about the movie and the user in the training data) was not available.

In order to determine the accuracy of a prediction of a particular model, it is
therefore necessary to use information about predictions from the other models.

\subsection{Singular Value Decomposition}

The most basic

Analyzing the singular vectors generated, the 


An analysis of the utility of these features showed that only the first couple
(which corresponded to the mean and bias of the data) had any significant
correlation with the 

\subsection{Denoising Auto-Encoder Decomposition}

One of my goals for this competition was to explore the use of deep neural network architectures.  The architecture that I choose was that of denoising autoencoders.

The goal of a standard auto encoder is to learn the identity function: two functions $f(\cdot)$ and $g(\cdot)$ such that $f(g(\mathbf{x})) = \mathbf{x}$.  Generally, in order to be interesting an auto-encoder will include some kind of restriction: for example, the number of dimensions in the range of $f$ is smaller than the number of dimentions in its domain.

In neural networks, the most commonly used formulation shares an activation matrix $\mathbf{W}$ between the forward and reverse directions:

$$
f(\mathbf{x}) = t(\mathbf{Wx} + \mathbf{b})
$$
$$
g(\mathbf{x}) = t(\mathbf{W}^T\mathbf{x} + \mathbf{c})
$$

The output of $f(x)$ is the \emph{encoded} form of the input $\mathbf{x}$, and the function $f$ is the \emph{encoder function}.  Similarly, $g$ is the \emph{decoder} function.  When we train the auto encoder over a 

In some circumstances, it may encode nearly the same information as in $\mathbf{x}$.  For example, if $t$ is the identity function and $\mathbf{W}$ is the left-singular matrix of a set of 

When $t$ is the identity function, this formulation reduces (essentially, see below) to the singular value decomposition.

When $t(\cdot)$ is a non-linear squashing function such as (in my case) $t(\mathbf{x}) = \tanh(\mathbf{x})$, we can use this neural network to learn a nonlinear decomposition.

Note that these auto-encoders can be stacked one on top of the other, so that we apply all of the forward functions one after the other and then all of the reverse directions in the opposite order.

In order to make the auto encoder generalize (and learn higher level features), we need to do one of two things:

\begin{itemize}
\item Add noise to the input, but train the network to reconstruct the non-noisy input (remove the noise).  This is where it becomes a ``denoising'' auto-encoder;
\item Create an information bottleneck, whereby the number of features from which the input is reconstructed is lower than the number of features in the input.
\end{itemize}

\subsubsection{Addition of noise}

\subsubsection{Information bottleneck}

In addition to adding noise as described above, I also created an information bottleneck.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics{auto-encoder}}
\caption{Denoising Auto Encoder used in solution}
\label{figure:autoencoder}
\end{center}
\vskip -0.2in
\end{figure} 

\section{Derived Features}

In order to make it easier for the classifiers to work, several features were derived from the outputs of the models:

\begin{itemize}
\item The minimum, maximum, mean and standard deviation of the model outputs;
\item For of the 10 highest ranking models:
  \begin{itemize}
    \item The minimum, maximum and mean value;
    \item The difference between the spread of values over these 10 models and the spread of values over all of the models;
    \item For each of the 10 models, the output of the model;
    \item For each of the 10 models, the number of standard deviations from the mean;
    \item The difference between the output of this model and the closest integer (for example, 4.513 which corresponds to 4513 in the data files would return 0.487);
    \item If a decomposition was done, the error of the reconstruction of this model by the decomposition;
  \end{itemize}
\item The output of the decomposition (SVD or Denoising Auto-Encoder), if there was any;
\item The overall RMS error of the decomposition on this example.

\end{itemize}


\section{Models}

There were three main models used in the entry:

\begin{itemize}
\item A multiple regression model;
\item A gated classifier model;
\item A deep neural network model.
\end{itemize}

In addition, several other models were tried:

\begin{itemize}
\item A boosting model
\item A standard classifier model
\end{itemize}

Each will be described briefly in the following sections.

\subsection{Multiple Regression Models}

\subsection{Gated Classifier Models}

It is likely that different models have differing circumstances under which they are accurate.  For example, a pure statistical model would probably be more accurate on those users and movies for which it had lots of data than on those for which it had little or no data.  An algorithm that looked at the words in the title of the movie would probably be less accurate on foreign titled films, and so on.

The goal of the gated classifier model was to learn which models were more likely to be accurate, and weight these disproportionately in the output.


\subsection{Deep Neural Network Model}

The denoising auto-encoders are trained with no knowledge of the target feature.  By adding an extra output layer or two and training the entire resulting network with backpropagation, the features that it has learned in its internal representation can be fine-tuned to help produce a target output (here, the label for the AUC or RMSE model).  This kind of model has already been used successfully for 

This model started with the denoising autoencoders

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics{deepnet}}
\caption{Deep Network Model.}
\label{figure:deepnet}
\end{center}
\vskip -0.2in
\end{figure} 

\section{Results Generation}

In order to generate results, a large number of predictors were trained individually based upon the models above.  Each of these predictors was trained on 80\% of the training data, with the other 20\% (always the same part) held out in order to train the final blending model.  Once each model was run, it created:

\begin{enumerate}
\item A blending results file containing the model's unbiased prediction for each entry of the 20\% of final blending data held out;
\item A submission results file containing the model's prediction for each entry of the scoring set (for which labels weren't available).
\end{enumerate}

Once all of the files were available, they were combined a final blender, and this result became the submitted output.

\subsection{Models Used}

There were four main classes of models that were used to contribute to the final score.

\subsubsection{Multiple Regression}

The multiple regression models turned out to be the most powerful models, particularly in the RMSE task.  This is largely due to their ability to reject noise due to their inherent smoothness, the regularization provided by ridge regression and the smoothing provided by the random selection of features and examples.

Table \ref{table:multiple-regression-models} describes the parameters for the different models.  In all cases, 500 separate regression models were combined (linear regression for the RMSE task; logistic regression for the AUC task) on 6,000 randomly selected examples.  On the small task, the number of features sampled and the decomposition order were set to 100; for the medium task 150 and for the large task 200.

\begin{table}
\caption{Multiple Regression Models Used}
\label{table:multiple-regression-models}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llc}
\hline
\abovespace\belowspace
Model & Decomposition & Extra Features \\
\hline
\abovespace
mr1   &        &         \\
mr2   & SVD    &         \\
mr3   & SVD    & $\surd$ \\
mr4   & DNAE1  &         \\
mr5   & DNAE1  & $\surd$ \\
mr6   & DNAE2  &         \\
mr7   & DNAE2  & $\surd$ \\
mr8   & DNAE3  &         \\
\belowspace
mr9   & DNAE3  & $\surd$ \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\subsubsection{Deep Neural Networks}

The very first results obtained from the deep neural network were extremely good, and placed quite high on the leaderboard.  However, all attempts to reproduce these results failed\footnote{The code that generated these results had a bug in it which caused the execution to be non-deterministic}: the training error of the deep net stopped improving far before the point observed on the first run.  The results from the deep net are included as they tend to be more diverse than the other models, but they did not contribute greatly to the final score.

Table \ref{table:deep-net-models} describes the deep network models used.  Each of these had an architecture with 250, 150, 100, 50 and 50 hidden units.  The 50 outputs of the autoencoder phase were added to the extra features which were fed into the last hidden layer.

\begin{table}
\caption{Deep Network Models Used}
\label{table:deep-net-models}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llc}
\hline
\abovespace\belowspace
Model & Decomposition & Extra Features \\
\hline
\abovespace
dn1   & DNAE2  & $\surd$  \\
dn2   & DNAE2  &   \\
dn3   & DNAE3  &   \\
\belowspace
dn4   & DNAE3  & $\surd$  \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\subsubsection{Gated Merger}

Several models of the gated merger were tried.  This model tended to perform reasonably well for the AUC task, but poorly for the RMSE task.  Presumably, this is because the two-stage nature of the model caused the noise to be amplified between the stages.

The models differed in which decomposition they used (no decomposition, the SVD or the denoising autoencoders), whether or not they used extra features, and the technique used for the final score once the confidence-modified values had been produced.


\begin{table}
\caption{Gated Merger Models Used}
\label{table:gated-models}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\hline
\abovespace\belowspace
Model & Decomp. & Blend RMSE & Blend AUC \\
\hline
\abovespace
gated   & SVD(200)  & LR & Rand Forest  \\
gated2  &           & LR & Rand Forest  \\
gated3  & DNAE1     & LR & Rand Forest  \\
gated4  & DNAE2     & LR & Rand Forest  \\
gated5  & DNAE3     & LR & Rand Forest  \\
gated6  & DNAE3     & Rand Forest & N/A  \\
\belowspace
gated7  & DNAE3     & Multi LR & N/A   \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsubsection{Classifier Models}

For the RMSE data, two classifier models were used.  One, \texttt{rtrees} used a random forest of 200 regression trees.  The other, \texttt{mclass}, learnt a binary classifier for each discrete movie rating (1, 2, 3, 4 or 5 stars) using a random forest of 500 decision trees, and combined these predictions using linear regression.  Neither model performed particularly well.


\subsection{Final Blending}

Final blending was performed using a cross-validation training on the 20\% held out data.  The held out data was broken into 10 folds, and 10 different multiple regression models were trained, each leaving out one fold.  The performance of the merged model on the entire 20\% held out was then evaluated.  The submission results were generated by running each of the 10 multiple regression models and averaging the results.

This strategy was adapted in order to reduce the impact that a ``rogue'' model (with a high error rate) would have on the final blend.  It is unlikely that performing (yet another) round of blending of already blended models would significantly improve the results.  Straight linear blending worked just as well, especially for the RMSE task.

The final blending provided a substantial improvement in the AUC task.  On the RMSE task, final blending was ineffectual: the score of the blended result was slightly worse than the best individual result\footnote{The submitted results were still the blended ones, however, as they should be more resistant to the selection bias in the validation set}.

This is probably due to there not being enough diversity in the models blended: there were only a few really distinct models, and these frequently used the same features (from the DNAE, the SVD and the derived features).

\subsection{Implementation}

In practice, this competition turned out (for me, anyway) to be as much about software engineering as about data mining.

The biggest reason for this was the amount of noise in the data compared to the size of the data itself.  The variation over even very sparesly parameterized models such as linear regression was extremely large, and for non-linear models (especially those using algorithms such as boosting) was even worse.  As a result, it was necessary to run each model as many times as possible over different random subsets of the data and average the results (for example, 5000 decision trees were trained for the random forest classifier in the gated model).

Due to the limited hardware resources available\footnote{One quad core ``hyperthreaded'' (8 virtual cores) desktop machine with 6GB of RAM, one dual core laptop with 2GB of RAM} and the large number of tasks, it was necessary that the software be both memory and CPU efficient.
The entire code was vectorized to take advantage of the vector unit, multi-threaded\footnote{On the desktop machine, 8 threads were run to fully exploit the hyper-threaded processor} and the bottlenecks were profiled and carefully optimized.  Single precision arithmetic was used wherever possible\footnote{It is frequently not possible.  For example, whenever accumulating a series of numbers it is necessary to accumulate in double precision even if the numbers being accumulated are only in single precision.} due to its advantage in execution speed.

In addition, parameters were saved using as small a precision as possible and care was taken not to duplicate memory when splitting datasets into training and validation sets.

The fact that there were six different tasks (AUC and RMSE for the small, medium and large datasets) also increased the amount of CPU time and engineering work required, as anything that was model or dataset-specific needed to be parameterized out and a method found to calculate the parameter for the task at hand.

In the end, about 5,000 lines of C++ code were written to handle the competition directly, and about 15,000 lines added to the underlying machine learning library (primarily the code to perform Ridge Regression and the denoising auto-encoder routines).  The entire set of results could be reproduced in about 24 hours on the desktop machine.

\subsection{Platform}

The software was developed on Linux.  The only significant external libraries used were LAPACK and BLAS for the linear algebra routines.

\subsection{Open Source}

The source code for this submission is available.  The machine learning library used to perform the heavy lifting is available at http://bitbucket.org/jeremy\_barnes/jml/.  The source code of the actual ausdm submission is abailable at http://github.com/jeremybarnes/ausdm.  Both are available under the Affero GNU Public License version 3.

Some of the data files used in the building of the results are available (the full set of files could not be provided as they are too large for the version control hosting service that is used).  These could be used to verify the results and to perform further experiments.  Please note that these files are only available to the AUSDM or NetFlix challenge participants, as they contain ranking values from the original NetFlix dataset that are not freely redistributable.

\section{Results}

% cat ../loadbuild/merged/L_rmse_official.txt.log | awk -v baseline=0.4419 'NF == 2 && $1 == "dn1" { go=1; ++iter; } !go { next; } $1 == "bias" || $1 == "combined" { go = 0; } iter < 11 { total[$1] += $2;  scores[$1,iter] = $2; } iter == 11 { score[$1] = $2; } END { for (model in total) { mean = total[model] / 10; var = 0;  for (j = 1;  j <= 10;  ++j) var += (scores[model,j] - mean) * (scores[model,j] - mean); printf("%-10s & %6.4f(%7.4f) & %7.4f$\\pm$%6.4f \\\\\n", model, score[model], baseline-score[model], mean, sqrt(var)); } printf("merged     & %6.4f(%7.4f) &         &        \\\\\n", score["combined"], baseline-score["combined"]); }'  | sort

We present the results in several tables, which show the performance of the component models and the blended result.  The table contains two columns of numerical information.  The first describes the error score of the model, with the lift (reduction in error) as compared with the baseline model.  The second shows the average blending weight of the model (over 10 folds) as well as the standard deviation in this value.

\subsection{AUC}

Looking first at the AUC scores.

%(cat ../loadbuild/merged/S_auc_official.txt.log; echo "next"; cat ../loadbuild/merged/M_auc_official.txt.log; echo "next"; cat ../loadbuild/merged/L_auc_official.txt.log; echo "next") | awk -v baseline1=0.0597 -v baseline2=0.3384 -v baseline3=0.1635 -f tabulate_results.awk | sort

\begin{table*}[t]
\caption{AUC Blending Results.  The lift is 1000 times the improvement over the baseline score.  The weight values are reported as mean $\pm$ standard deviation.}
\label{table:auc-results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|rr r|rr r|rr r}
\hline
\abovespace\belowspace
Task & \multicolumn{3}{|c}{Small} & \multicolumn{3}{|c}{Medium} & \multicolumn{3}{|c}{Large} \\
Model
& AOC & Lift & Weight 
& AOC & Lift & Weight 
& AOC & Lift & Weight \\
\hline
\abovespace
dn1        & 0.0585 &   1.2 & -0.36$\pm$0.32& 0.3299 &   8.5 &  0.04$\pm$0.23& 0.1581 &   5.4 & -0.41$\pm$0.20 \\ 
dn2        & 0.0611 &  -1.4 &  0.20$\pm$0.19& 0.3431 &  -4.7 & -0.51$\pm$0.22& 0.1634 &   0.1 & -0.05$\pm$0.21 \\ 
dn3        & 0.0611 &  -1.4 &  0.19$\pm$0.17& 0.3432 &  -4.8 & -0.49$\pm$0.19& 0.1634 &   0.1 & -0.60$\pm$0.23 \\ 
dn4        & 0.0585 &   1.2 & -0.34$\pm$0.30& 0.3300 &   8.4 &  0.05$\pm$0.24& 0.1581 &   5.4 & -0.83$\pm$0.31 \\ 
\abovespace
gated      & 0.0565 &   3.2 &  0.80$\pm$0.23& 0.3239 &  14.5 &  0.60$\pm$0.25& 0.1497 &  13.8 &  1.09$\pm$0.27 \\ 
gated2     & 0.0584 &   1.3 &  0.26$\pm$0.24& 0.3318 &   6.6 & -0.08$\pm$0.27& 0.1528 &  10.7 &  0.32$\pm$0.24 \\ 
gated3     & 0.0592 &   0.5 & -0.55$\pm$0.35& 0.3282 &  10.2 &  0.09$\pm$0.29& 0.1524 &  11.1 &  0.19$\pm$0.32 \\ 
gated4     & 0.0586 &   1.1 & -0.02$\pm$0.29& 0.3303 &   8.1 & -0.26$\pm$0.44& 0.1528 &  10.7 & -0.64$\pm$0.33 \\ 
gated5     & 0.0580 &   1.7 &  0.37$\pm$0.14& 0.3264 &  12.0 &  0.11$\pm$0.31& 0.1519 &  11.6 &  0.32$\pm$0.13 \\ 
\abovespace
mr1        & 0.0639 &  -4.2 &  0.23$\pm$0.33& 0.3208 &  17.6 &  0.47$\pm$0.47& 0.1581 &   5.4 & -0.08$\pm$0.39 \\ 
mr2        & 0.0633 &  -3.6 &  0.18$\pm$0.33& 0.3203 &  18.1 &  0.46$\pm$0.49& 0.1584 &   5.1 & -0.88$\pm$0.30 \\ 
mr3        & 0.0575 &   2.2 &  0.57$\pm$0.51& 0.3174 &  21.0 &  0.03$\pm$0.51& 0.1499 &  13.6 &  1.84$\pm$0.69 \\ 
mr4        & 0.0573 &   2.4 &  0.39$\pm$0.27& 0.3196 &  18.8 &  0.82$\pm$0.34& 0.1508 &  12.7 & -0.75$\pm$0.38 \\ 
mr5        & 0.0574 &   2.3 & -0.29$\pm$0.29& 0.3128 &  25.6 &  0.80$\pm$0.44& 0.1485 &  15.0 &  1.58$\pm$0.47 \\ 
mr6        & 0.0573 &   2.4 &  0.51$\pm$0.30& 0.3201 &  18.3 &  0.61$\pm$0.22& 0.1505 &  13.0 &  0.13$\pm$0.29 \\ 
mr7        & 0.0573 &   2.4 & -0.21$\pm$0.25& 0.3135 &  24.9 &  0.65$\pm$0.42& 0.1485 &  15.0 &  0.74$\pm$0.43 \\ 
mr8        & 0.0572 &   2.5 &  0.90$\pm$0.60& 0.3186 &  19.8 & -0.06$\pm$0.30& 0.1500 &  13.5 &  2.04$\pm$0.57 \\ 
mr9        & 0.0574 &   2.3 & -0.04$\pm$0.16& 0.3146 &  23.8 &  0.24$\pm$0.24& 0.1484 &  15.1 &  0.58$\pm$0.66 \\ 
%\abovespace
%bias       &        &       & -1.09$\pm$0.46&        &       & -2.10$\pm$0.34&        &       & -2.57$\pm$0.32 \\ 
\abovespace\belowspace
combined   & 0.0571 &   2.6 &  & 0.3144 &  24.0 &  & 0.1461 &  17.4 &   \\ 
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


%cat ../loadbuild/merged/S_rmse_official.txt.log; echo "next"; cat ../loadbuild/merged/M_rmse_official.txt.log; echo "next"; cat ../loadbuild/merged/L_rmse_official.txt.log; echo "next") | awk -v baseline1=0.4398 -v baseline2=0.4307 -v baseline3=0.4419 -f tabulate_results.awk | sort

\begin{table*}[b]
\caption{RMSE Blending Results}
\label{table:rmse-results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|rr r|rr r|rr r}
\hline
\abovespace\belowspace
Task & \multicolumn{3}{|c}{Small} & \multicolumn{3}{|c}{Medium} & \multicolumn{3}{|c}{Large} \\
Model
& RMSE & Lift & Weight 
& RMSE & Lift & Weight 
& RMSE & Lift & Weight \\
\hline
\abovespace
dn1        & 0.4422 &  -2.4 &  0.02$\pm$0.01& 0.4332 &  -2.5 &  0.04$\pm$0.02& 0.4442 &  -2.3 & -0.06$\pm$0.03 \\ 
dn2        & 0.4485 &  -8.7 &  0.02$\pm$0.03& 0.4407 & -10.0 &  0.03$\pm$0.02& 0.4495 &  -7.6 & -0.02$\pm$0.04 \\ 
dn3        & 0.4486 &  -8.8 &  0.02$\pm$0.04& 0.4409 & -10.2 &  0.03$\pm$0.03& 0.4497 &  -7.8 &  0.02$\pm$0.07 \\ 
dn4        & 0.4421 &  -2.3 &  0.02$\pm$0.01& 0.4332 &  -2.5 &  0.03$\pm$0.02& 0.4442 &  -2.3 & -0.06$\pm$0.04 \\ 
\abovespace
gated      & 0.4500 & -10.2 &  0.07$\pm$0.04& 0.4462 & -15.5 &  0.02$\pm$0.04& 0.4418 &   0.1 &  0.09$\pm$0.04 \\ 
gated2     & 0.4409 &  -1.1 &  0.08$\pm$0.04& 0.4315 &  -0.8 &  0.07$\pm$0.04& 0.4414 &   0.5 &  0.09$\pm$0.04 \\ 
gated3     & 0.4554 & -15.6 & -0.04$\pm$0.03& 0.4420 & -11.3 &  0.02$\pm$0.04& 0.4428 &  -0.9 &  0.05$\pm$0.04 \\ 
gated4     & 0.4490 &  -9.2 &  0.03$\pm$0.03& 0.4393 &  -8.6 &  0.00$\pm$0.03& 0.4419 &   0.0 &  0.04$\pm$0.05 \\ 
gated5     & 0.4620 & -22.2 &  0.03$\pm$0.03& 0.4487 & -18.0 & -0.04$\pm$0.04& 0.4460 &  -4.1 &  0.01$\pm$0.02 \\ 
gated6     & 0.4421 &  -2.3 &  0.03$\pm$0.04& 0.4317 &  -1.0 &  0.03$\pm$0.04& 0.4421 &  -0.2 &  0.02$\pm$0.05 \\ 
gated7     & 0.4394 &   0.4 &  0.04$\pm$0.02& 0.4295 &   1.2 &  0.04$\pm$0.03& 0.4405 &   1.4 & -0.05$\pm$0.10 \\ 
\abovespace
mr1        & 0.4379 &   1.9 &  0.07$\pm$0.02& 0.4277 &   3.0 &  0.10$\pm$0.02& 0.4386 &   3.3 &  0.13$\pm$0.04 \\ 
mr2        & 0.4377 &   2.1 &  0.08$\pm$0.01& 0.4275 &   3.2 &  0.10$\pm$0.04& 0.4387 &   3.2 &  0.10$\pm$0.02 \\ 
mr3        & 0.4377 &   2.1 &  0.07$\pm$0.02& 0.4276 &   3.1 &  0.09$\pm$0.03& 0.4386 &   3.3 &  0.11$\pm$0.03 \\ 
mr4        & 0.4382 &   1.6 &  0.06$\pm$0.03& 0.4278 &   2.9 &  0.08$\pm$0.03& 0.4386 &   3.3 &  0.10$\pm$0.06 \\ 
mr5        & 0.4379 &   1.9 &  0.06$\pm$0.02& 0.4277 &   3.0 &  0.08$\pm$0.04& 0.4386 &   3.3 &  0.10$\pm$0.06 \\ 
mr6        & 0.4379 &   1.9 &  0.07$\pm$0.02& 0.4277 &   3.0 &  0.09$\pm$0.03& 0.4386 &   3.3 &  0.11$\pm$0.02 \\ 
mr7        & 0.4377 &   2.1 &  0.07$\pm$0.02& 0.4278 &   2.9 &  0.06$\pm$0.04& 0.4385 &   3.4 &  0.15$\pm$0.09 \\ 
mr8        & 0.4377 &   2.1 &  0.07$\pm$0.02& 0.4275 &   3.2 &  0.10$\pm$0.04& 0.4389 &   3.0 &  0.07$\pm$0.02 \\ 
mr9        & 0.4379 &   1.9 &  0.06$\pm$0.01& 0.4278 &   2.9 &  0.05$\pm$0.05& 0.4389 &   3.0 &  0.05$\pm$0.03 \\ 
\abovespace
mclass     & 0.4398 &   0.0 &  0.06$\pm$0.05& 0.4319 &  -1.2 &  0.02$\pm$0.03& 0.4417 &   0.2 & -0.00$\pm$0.04 \\ 
rtrees     & 0.4411 &  -1.3 &  0.05$\pm$0.05& 0.4321 &  -1.4 &  0.02$\pm$0.04& 0.4427 &  -0.8 & -0.05$\pm$0.05 \\ 
%\abovespace
%bias       &        &       &  0.00$\pm$0.02&        &       & -0.02$\pm$0.01&        &       & -0.00$\pm$0.01 \\ 
\abovespace\belowspace
combined   & 0.4381 &   1.7 &  & 0.4277 &   3.0 &  & 0.4385 &   3.4 &   \\ 
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\section{Discussion}

\subsection{RMSE Task}

The overall lift achieved for the RMSE task was very poor, and far below the level where a person using the system would notice a difference in the quality of the recommendations.  Even the highest placed teams on the leaderboard achieved a lift of 10\% or so more than was achieved here.

This phenomena is probably explained by the use of a RMSE metric, and the amount of noise in the data.  The RMSE metric penalizes very heavily incorrect predictions at the extreme ends of the scale: a prediction of 1 has 4 times the potential error.

\subsection{Difficulty Values of Rows}

It is instructive to break the dataset into different types of rows, based upon the ease with which a prediction can be made.  For RMSE, we could define a row to be \emph{impossible} if the predictions of all of the models are closer to the target value than the

\begin{table}[t]
\caption{RMSE large set broken down by row difficulty.  In the last column, we see that 1/3 of the MSE comes from the impossible positions, which account for just 5.7\% of the data.}
\label{rowtypesrmse}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrr}
\hline
\abovespace\belowspace
Category & Freq & Avg MSE & Total MSE \\
\hline
\abovespace
Automatic     & 0.329 & 0.019 & 0.0063 \\
Possible      & 0.614 & 0.198 & 0.1216 \\
\belowspace
Impossible    & 0.057 & 1.189 & 0.0678 \\
\hline
Total         & 1.000 & 0.195 & 0.1953 \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}




\subsection{Black Box Model}


\subsection{}


\subsection{Directions for Further Investigation}

\begin{itemize}
\item Non-linear but noise resistant regression models
\end{itemize}


\section{Conclusion}

The limitations of the black-box model of ensembling, especially using the RMSE loss function to measure performance, are clear from this competition.  Judging from the leaderboard on the small task, it is unlikely that any team managed to achieve a significant improvement over the baseline.

The RMSE measure used has the effect of clustering the predictions around the dataset mean.  This is especially true for the lower rankings of 1, which are already exceedingly rare.

An improved model of ensembling was proposed, whereby each model in the ensemble provides not only a prediction of the target function, but a list of features that can be used to determine when the model is likely to be inaccurate.  This model requires further work on the part of the ensemble builders to provide this information, but would allow the ensembling method to have significantly more leverage.

% Acknowledgements should only appear in the accepted version. 
\section*{Acknowledgments} 

Thanks to Phil Brierley for organizing and running the competition.

The \LaTeX style file for this report is based upon that for ICML 2009, by P Langley, Terran Lane, Jennifer Dy, Kristian Kersting and Ricardo Silva.


\bibliography{report}
\bibliographystyle{mlapa}

\end{document} 


jeremy@jeremy-desktop:~/projects/ausdm$ for n in 1 2 3 4 5 7 10 12 15 17 20 25 30 40 50; do echo -n "$n "; ./build/x86_64/bin/ausdm -T 0.20 -t rmse blender.num_models=$n 2>/dev/null | tail -n1; done
1 0.8692
2 0.8659
3 0.8643
4 0.8627
5 0.8602
7 0.8597
10 0.8597
12 0.8592
15 0.8591
17 0.8594
20 0.8596
25 0.8603
30 0.8605
40 0.8606
50 0.8608

jeremy@jeremy-desktop:~/projects/ausdm$ for n in 1 2 3 4 5 7 10 12 15 17 20 25 30 40 50; do echo -n "$n "; ./build/x86_64/bin/ausdm -T 0.20 -t auc blender.num_models=$n 2>/dev/null | tail -n1; done
1 0.8828
2 0.8869
3 0.8867
4 0.8860
5 0.8871
7 0.8867
10 0.8865
12 0.8862
15 0.8858
17 0.8859
20 0.8856
25 0.8854
30 0.8853
40 0.8851
50 0.8851

So, 15 models seems to be good for RMSE and 5 for AUC



-------

jeremy@jeremy-desktop:~/projects/ausdm$ for n in 1 2 3 4 5 7 10 12 15 17 20 25 30 40 50; do echo -n "$n "; ./build/x86_64/bin/ausdm -T 0.20 -t rmse blender.num_models=$n -r 10 2>/dev/null | tail -n1; done
1 0.8761 +/- 0.0151
2 0.8706 +/- 0.0144
3 0.8688 +/- 0.0142
4 0.8681 +/- 0.0145
5 0.8675 +/- 0.0139
7 0.8669 +/- 0.0139
10 0.8664 +/- 0.0140
12 0.8662 +/- 0.0141
15 0.8659 +/- 0.0140
17 0.8659 +/- 0.0140
20 0.8656 +/- 0.0140
25 0.8658 +/- 0.0140
30 0.8661 +/- 0.0140
40 0.8664 +/- 0.0139
50 0.8666 +/- 0.0139


jeremy@jeremy-desktop:~/projects/ausdm$ for n in 1 2 3 4 5 7 10 12 15 17 20 25 30 40 50; do echo -n "$n "; ./build/x86_64/bin/ausdm -T 0.20 -t auc blender.num_models=$n -r 10 2>/dev/null | tail -n1; done
1 0.8750 +/- 0.0090
2 0.8788 +/- 0.0081
3 0.8792 +/- 0.0080
4 0.8786 +/- 0.0079
5 0.8797 +/- 0.0075
7 0.8804 +/- 0.0072
10 0.8800 +/- 0.0074
12 0.8799 +/- 0.0074
15 0.8798 +/- 0.0075
17 0.8796 +/- 0.0075
20 0.8794 +/- 0.0075
25 0.8789 +/- 0.0077
30 0.8783 +/- 0.0079
40 0.8777 +/- 0.0080
50 0.8774 +/- 0.0081

Better results (more trials): 15-20 for RMSE, 7 for AUC


Boosting:

./build/x86_64/bin/ausdm -T 0.20 -t auc blender.num_models=5 -r 10

0.8779 +/- 0.0077

Not much good f as a first try...



With impossible examples excluded:

./build/x86_64/bin/ausdm -T 0.20 -t auc blender.num_models=5 -r 10

0.8776 +/- 0.0080

Still not much good...


With early stopping:

0.8772 +/- 0.0077

Even worse, not encouraging


With selection via margin:

0.8773 +/- 0.0076

Not any better!


Minor tweaks:

0.8779 +/- 0.0080

------------------------

Gated solution, try 1:

./build/x86_64/bin/ausdm -T 0.20 -t auc -r 10

0.8799 +/- 0.0074


With bug fixes:

0.8804 +/- 0.0074


Predicting actual margin (but using logit) (removed)

0.8801 +/- 0.0074


20 inputs

0.8798 +/- 0.0076

Back to 10 inputs, with extra statistical features:

0.8807 +/- 0.0075

Added a few extra along the same lines:

0.8807 +/- 0.0075

Removed decomposition features:

0.8805 +/- 0.0074

But this is in spite of the fact that the confidence functions got much, much worse:

Before:

trial 9
200 models... 15000 rows... n = 12000 m = 200 nvalues = 200
model 123: before 0.873762/0.256815 after 0.366578/0.535418
model 71: before 0.870887/0.24351 after 0.340936/0.541159
model 81: before 0.870203/0.24804 after 0.35414/0.536658
model 7: before 0.870063/0.260856 after 0.380631/0.544828
model 65: before 0.86898/0.25535 after 0.372716/0.547804
model 46: before 0.868584/0.261021 after 0.384278/0.541895
model 140: before 0.871122/0.25586 after 0.379631/0.533441
model 88: before 0.871134/0.252219 after 0.374164/0.532919
model 150: before 0.871441/0.23935 after 0.339959/0.533688
model 167: before 0.87121/0.260366 after 0.383654/0.540394


After:

trial 9
200 models... 15000 rows... n = 12000 m = 200 nvalues = 200
model 88: before 0.871134/0.252219 after 0.539543/0.308744
model 71: before 0.870887/0.24351 after 0.47926/0.326779
model 123: before 0.873762/0.256815 after 0.534246/0.299965
model 140: before 0.871122/0.25586 after 0.544026/0.309047
model 7: before 0.870063/0.260856 after 0.537814/0.337856
model 81: before 0.870203/0.24804 after 0.515196/0.318835
model 65: before 0.86898/0.25535 after 0.52039/0.337979
model 46: before 0.868584/0.261021 after 0.541253/0.335966
model 167: before 0.87121/0.260366 after 0.563316/0.308776
model 150: before 0.871441/0.23935 after 0.516603/0.288806



--------------

Attempt to use a simple linear blending scheme:

0.8802 +/- 0.0086


Same scheme but with some extra features (including those necessary to emulate the previous system):

0.8808 +/- 0.0087



-------------

RMSE with numeric problems fixed up (I think!):

0.8683 +/- 0.0133

With them really fixed up:

0.8689 +/- 0.0139

Very disappointing!

Same for AUC:

0.8807 +/- 0.0081

With 20 weak learners:

0.8785 +/- 0.0073

So still not there yet...

Submission: (gated top 10 (fixed))

Gini = 0.8755559356638
Success - rmse = 882.36039813249

-------------------

AUC with extra features:

0.8798 +/- 0.0085

Beginning to suspect something wrong (systematic biasing); shouldn't be going down like this!

RMSE with same extra features:

0.8706 +/- 0.0144

So just adding good features gets worse for RMSE as well.

----------------------

AUC with bagged boosted trees for blending:

0.8795 +/- 0.0071

And with a GLZ:

0.8794 +/- 0.0083

With decomposition trained on 1/3 of the data:

auc: 0.8738 +/- 0.0087

Ouch!

Fixed up to use the correct decomposition (I think):

0.8763 +/- 0.0088

Still not great...

And back to using full data for the decomposition:

0.8792 +/- 0.0086

Using the full, full, full set of data for the decomposition:

0.8797 +/- 0.0091


---------------------


Using a neural network to perform blending:

0.8814 +/- 0.0079

Now we're talking!

Submitted:


Success
Gini = 0.87422844737031
AUC = 0.93711422368515


-----------------------

Before using boosting around everything else:

scores: [ 0.117628 0.126624 0.123454 0.134264 0.135271 0.131286 0.131046 0.137431 0.110226 0.119526 ]
0.1267 +/- 0.0088

With boosting (same as before, on top of boosted bagged decision trees):

scores: [ 0.119883 0.122842 0.124331 0.129082 0.136901 0.134772 0.134071 0.136432 0.108208 0.122468 ]
0.1269 +/- 0.0091

Success
Gini = 0.87000892970578
AUC = 0.93500446485289

With GLZ:

scores: [ 0.117289 0.113771 0.113929 0.125466 0.127324 0.130649 0.124218 0.124263 0.105995 0.116004 ]
0.1199 +/- 0.0077

for n in 1 2 3 4 5 7 10 12 15 17 20 25 30 40 50; do echo -n "$n "; ./build/x86_64/bin/ausdm --no-decomposition -T 0.20 -t auc auc.type=linear auc.mode=best_n auc.num_models=$n 2>/dev/null | tail -n1; done
1 0.1218 +/-    nan
2 0.1200 +/-    nan
3 0.1215 +/-    nan
4 0.1200 +/-    nan
5 0.1211 +/-    nan
7 0.1201 +/-    nan
10 0.1193 +/-    nan
12 0.1194 +/-    nan
15 0.1195 +/-    nan
17 0.1199 +/-    nan
20 0.1206 +/-    nan
25 0.1205 +/-    nan
30 0.1208 +/-    nan
40 0.1206 +/-    nan
50 0.1212 +/-    nan



jeremy@jeremy-desktop:~/projects/ausdm$ for n in 1 2 3 4 5 7 10 12 15 17 20 25 30 40 50; do echo -n "$n "; ./build/x86_64/bin/ausdm --no-decomposition -T 0.20 -t auc -r 10 auc.type=linear auc.mode=best_n auc.num_models=$n 2>/dev/null | tail -n1; done
1 0.1252 +/- 0.0090
2 0.1222 +/- 0.0082
3 0.1218 +/- 0.0081
4 0.1210 +/- 0.0083
5 0.1213 +/- 0.0082
7 0.1202 +/- 0.0077
10 0.1200 +/- 0.0074
12 0.1201 +/- 0.0074
15 0.1201 +/- 0.0074
17 0.1203 +/- 0.0074
20 0.1207 +/- 0.0075
25 0.1211 +/- 0.0077
30 0.1217 +/- 0.0079
40 0.1223 +/- 0.0079
50 0.1226 +/- 0.0081



